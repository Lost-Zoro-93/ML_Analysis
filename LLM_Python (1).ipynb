{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3459317d-f68a-419c-9a48-c6c2fc126457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visha\\Downloads\\LLM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3c0dcbb-9cb1-48c2-a834-62bbc1b73d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol char: 21202\n",
      "An Occurrence at Owl Creek Bridge\n",
      "\n",
      "by Ambrose Bierce\n",
      "\n",
      "THE MILLENNIUM FULCRUM EDITION, 1988\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I\n",
      "\n",
      "\n",
      "A man stood upon a railroad bridge in northern Alabama, looking down\n",
      "into the swift water twenty feet below. The man's hands were behind his\n",
      "back, the wrists bound with a cord. A rope closely encircled his neck.\n",
      "It was attached to a stout cross-timber above his head and the slack\n",
      "fell to the level of his knees. Some loose boards laid upon the ties\n",
      "supporting the rails of the railway supplied a footing for him and his\n",
      "executioners-two private soldiers of the Federal army, directed by a\n",
      "sergeant w\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "with open(\"An Occurrence at Owl Creek Bridge.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Normalize accents\n",
    "raw_text = unicodedata.normalize(\"NFKD\", raw_text)\n",
    "\n",
    "# Replace curly punctuation and other special symbols\n",
    "def clean_smart_punctuation(text):\n",
    "    # Translation table: maps ordinals (Unicode code points) to replacement characters\n",
    "    replacements = {\n",
    "        ord('“'): '\"', ord('”'): '\"',\n",
    "        ord('‘'): \"'\", ord('’'): \"'\",\n",
    "        ord('—'): '-', ord('–'): '-',\n",
    "        ord('…'): '...',\n",
    "        ord('″'): '\"', ord('‛'): \"'\",  # less common variants\n",
    "        ord('‹'): '<', ord('›'): '>',\n",
    "        ord('«'): '\"', ord('»'): '\"',\n",
    "    }\n",
    "    return text.translate(replacements)\n",
    "\n",
    "raw_text = clean_smart_punctuation(raw_text)\n",
    "\n",
    "print(\"tol char:\", len(raw_text))\n",
    "print(raw_text[:599])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80c7281b-a9d1-437d-a5c6-9ec84465d2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello,', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'hands', ' ', 'on', ' ', 'practice', ' ', 'of', ' ', 'llm', ' ', 'from', ' ', 'scratch!!.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"hello, this is a hands on practice of llm from scratch!!.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ffcf2a7-10e7-419a-ac47-a1bc1830141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', '', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'hands', ' ', 'on', ' ', 'practice', ' ', 'of', ' ', 'llm', ' ', 'from', ' ', 'scratch', '!', '', '!', '', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([.,!]|\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3629e11-e8fd-4bd3-8bf5-f18c11ff5253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'this', 'is', 'a', 'hands', 'on', 'practice', 'of', 'llm', 'from', 'scratch', '!', '!', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac9f0224-f757-479c-a2eb-af56f537ec3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      ";\n",
      "\"\n",
      ",\n",
      ":\n",
      ".\n",
      "'\n",
      "!\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "special_chars = re.findall(r'[^a-zA-Z0-9\\s]', raw_text)\n",
    "unique_special_chars = list(set(special_chars))\n",
    "\n",
    "#print(\"All special characters:\", special_chars)\n",
    "#print(\"Unique special characters:\", unique_special_chars)\n",
    "for char in unique_special_chars:\n",
    "    print(char)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "274a2f97-140c-452a-b69c-135ed2654bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['An', 'Occurrence', 'at', 'Owl', 'Creek', 'Bridge', 'by', 'Ambrose', 'Bierce', 'THE', 'MILLENNIUM', 'FULCRUM', 'EDITION', ',', '1988', 'I', 'A', 'man', 'stood', 'upon', 'a', 'railroad', 'bridge', 'in', 'northern', 'Alabama', ',', 'looking', 'down', 'into']\n"
     ]
    }
   ],
   "source": [
    "pre_processed = re.split(r'([.:!,\\'\";\\-?]+|\\s)', raw_text)\n",
    "pre_processed = [item.strip() for item in pre_processed if item.strip()]\n",
    "print(pre_processed[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "833289b7-7022-45c2-9c86-c8c19eb83edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340\n"
     ]
    }
   ],
   "source": [
    "print(len(pre_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba17cb40-58d0-439d-8ece-9f0aac26f8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1333\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(pre_processed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a9e776b-6cf4-43bb-8300-29b92a517d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('!\"', 1)\n",
      "('!-', 2)\n",
      "('!...', 3)\n",
      "('\"', 4)\n",
      "(\"'\", 5)\n",
      "(',', 6)\n",
      "(',\"', 7)\n",
      "('-', 8)\n",
      "('.', 9)\n",
      "('.\"', 10)\n",
      "('1988', 11)\n",
      "(':', 12)\n",
      "(';', 13)\n",
      "('?\"', 14)\n",
      "('A', 15)\n",
      "('AEolian', 16)\n",
      "('About', 17)\n",
      "('Ah', 18)\n",
      "('Aim', 19)\n",
      "('Alabama', 20)\n",
      "('All', 21)\n",
      "('Although', 22)\n",
      "('Ambrose', 23)\n",
      "('An', 24)\n",
      "('And', 25)\n",
      "('As', 26)\n",
      "('At', 27)\n",
      "('Attention', 28)\n",
      "('Being', 29)\n",
      "('Beyond', 30)\n",
      "('Bierce', 31)\n",
      "('Bravo', 32)\n",
      "('Bridge', 33)\n",
      "('But', 34)\n",
      "('By', 35)\n",
      "('Circumstances', 36)\n",
      "('Company', 37)\n",
      "('Corinth', 38)\n",
      "('Creek', 39)\n",
      "('DIMINUENDO', 40)\n",
      "('Death', 41)\n",
      "('Doubtless', 42)\n",
      "('EDITION', 43)\n",
      "('Encompassed', 44)\n",
      "('Evidently', 45)\n",
      "('Excepting', 46)\n",
      "('FULCRUM', 47)\n",
      "('Farquhar', 48)\n",
      "('Federal', 49)\n",
      "('Fire', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff32a211-fe0a-48f0-8a5b-98c6f4235eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        pre_processed = re.split(r'([.:!,\\'\";\\-?]+|\\s)', text)\n",
    "        pre_processed = [item.strip() for item in pre_processed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in pre_processed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        #Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([.:!,\\'\";\\-?])',r'\\1', text)\n",
    "        return text\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ef5c24c-f7d6-4c1b-b749-ae0a8ac1a811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[179, 6, 1177, 1329, 226, 1314, 105, 312, 9, 15, 990, 279, 416, 620, 798, 9, 62, 1281, 169, 1208, 105, 1118, 319, 8, 1204, 107, 620, 606, 135, 1177, 1063, 478, 1208, 1177, 714, 827, 620, 689, 9, 84, 736, 218]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"back, the wrists bound with a cord. A rope closely encircled his neck.\n",
    "It was attached to a stout cross-timber above his head and the slack\n",
    "fell to the level of his knees. Some loose boards \"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8627e717-13e4-40c6-94b7-592478c42d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a back, a the a wrists a bound a with a cord. A rope a closely a encircled a his a neck. It was attached to a stout cross- timber above his head and the slack fell to the level of his knees. Some loose boards'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e0cff4b-17ca-4a62-9ddf-c78e2e444d22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'you'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mencode(text))\n",
      "Cell \u001b[1;32mIn[23], line 9\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m pre_processed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([.:!,\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m-?]+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m      8\u001b[0m pre_processed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m pre_processed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m pre_processed]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'you'"
     ]
    }
   ],
   "source": [
    "text = \"do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f702c45-4dc7-4c37-80a0-16fbdc92038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(pre_processed)))\n",
    "all_tokens.extend([\"<|endoftext|>\",\"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0fcb6-294b-4bce-be72-0245903c7b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2512c67-2d9f-4ef4-befb-e298ec267732",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e4852c-02c1-43ea-9804-0b8b0b6ac94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__ (self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        pre_processed = re.split(r'([.:!,\\'\";\\-?]+|\\s)', text)\n",
    "        pre_processed = [item.strip() for item in pre_processed if item.strip()]\n",
    "        pre_processed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in pre_processed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in pre_processed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([.:!,\\'\";\\-?])',r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663aee13-db29-4ab3-bf75-5bea00a9ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e670b7-dbc7-4fd6-8e15-7a8531f9435f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23c7c6-2c1f-4f3f-93bf-5b37e383e80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40d0711-604b-4b82-a1ce-28bd27b6aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\visha\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\visha\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\visha\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\visha\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\visha\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\visha\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\visha\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bae6a5b7-8b2b-4af2-9459-cc4ba0a923f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\",importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef854b74-ddd6-40d9-9dc2-5fe7832b27a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36e09858-fede-456d-b6ee-39bed09f50bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1212\t'This'\n",
      "318\t' is'\n",
      "257\t' a'\n",
      "6291\t' sample'\n",
      "6827\t' sentence'\n",
      "220\t' '\n",
      "50256\t'<|endoftext|>'\n",
      "543\t' which'\n",
      "1312\t' i'\n",
      "1101\t\"'m\"\n",
      "1262\t' using'\n",
      "329\t' for'\n",
      "428\t' this'\n",
      "1332\t' test'\n",
      "1057\t' run'\n",
      "13\t'.'\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"This is a sample sentence <|endoftext|> which i'm using for this test run.\"\n",
    ")\n",
    "# Encode the text with special tokens\n",
    "token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "# Decode each token ID individually to get the token string\n",
    "tokens = [tokenizer.decode([token_id]) for token_id in token_ids]\n",
    "\n",
    "# Print token ID and corresponding token string\n",
    "for token_id, token_str in zip(token_ids, tokens):\n",
    "    print(f\"{token_id}\\t{repr(token_str)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c554891-3f3c-4e08-bdbc-f6e441f4b2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample sentence <|endoftext|> which i'm using for this test run.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(token_ids)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b162e45-2c5c-4cb2-8245-c9c5b643d587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23205\t'love'\n",
      "12379\t' da'\n",
      "8591\t' la'\n",
      "474\t' j'\n",
      "1453\t'ee'\n",
      "85\t'v'\n",
      "342\t'ith'\n",
      "321\t'am'\n",
      "love da la jeevitham\n"
     ]
    }
   ],
   "source": [
    "token_ids1 = tokenizer.encode(\"love da la jeevitham\")\n",
    "# Decode each token ID individually to get the token string\n",
    "tokens = [tokenizer.decode([token_id]) for token_id in token_ids1]\n",
    "\n",
    "# Print token ID and corresponding token string\n",
    "for token_id, token_str in zip(token_ids1, tokens):\n",
    "    print(f\"{token_id}\\t{repr(token_str)}\")\n",
    " \n",
    "string = tokenizer.decode(token_ids1)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00b7411-e023-4e75-8bb9-b2f968ff991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids1 = tokenizer.encode(\"love da la jeevitham\")\n",
    "# Decode each token ID individually to get the token string\n",
    "tokens = [tokenizer.decode([token_id]) for token_id in token_ids1]\n",
    "\n",
    "# Print token ID and corresponding token string\n",
    "for token_id, token_str in zip(token_ids1, tokens):\n",
    "    print(f\"{token_id}\\t{repr(token_str)}\")\n",
    " \n",
    "string = tokenizer.decode(token_ids1)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec74c2e9-7254-4e69-9b88-04a4336c0f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for GPT2 is: 50257\n",
      "The vocabulary size for GPT3 is: 50281\n",
      "The vocabulary size for GPT4 is: 100277\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encodings for GPT-2, GPT-3, and GPT-4\n",
    "encodings = {\n",
    "    \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
    "    \"gpt3\": tiktoken.get_encoding(\"p50k_base\"),  # Commonly associated with GPT-3 models\n",
    "    \"gpt4\": tiktoken.get_encoding(\"cl100k_base\")  # Used for GPT-4 and later versions\n",
    "}\n",
    "\n",
    "# Get the vocabulary size for each encoding\n",
    "vocab_sizes = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
    "\n",
    "# Print the vocabulary sizes\n",
    "for model, size in vocab_sizes.items():\n",
    "    print(f\"The vocabulary size for {model.upper()} is: {size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "855d1717-356b-40e9-9fc0-2c8972f1a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\visha\\Downloads\\LLM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6328321e-66bb-48a5-9b6e-d478bffc3d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5220\n"
     ]
    }
   ],
   "source": [
    "with open(\"An Occurrence at Owl Creek Bridge.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1956b879-856e-40d1-a1ac-2996c86361d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "09adcfcc-4282-4cfc-8aa6-c2839e5c2907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [14622, 1660, 8208, 3625]\n",
      "y:      [1660, 8208, 3625, 2174]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of the input\n",
    "#The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) \n",
    "#to predict the next word in the sequence. \n",
    "#The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a6136ae3-62d4-49b2-b606-24a90360b69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14622] ----> 1660\n",
      "[14622, 1660] ----> 8208\n",
      "[14622, 1660, 8208] ----> 3625\n",
      "[14622, 1660, 8208, 3625] ----> 2174\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "73c2f0d9-d824-418a-8c6c-2bbde02b6ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " swift ---->  water\n",
      " swift water ---->  twenty\n",
      " swift water twenty ---->  feet\n",
      " swift water twenty feet ---->  below\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c1c00-0cca-4ef6-a58c-169af1d5629a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fd398-05b6-4e47-8769-4179e15cdec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf0f2af-7f7f-4777-8485-bd5809821e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2e8307-9ab8-4712-badf-11dbd0f97f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f8f05-18b0-469b-9984-4b5a4821d0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
